"""
Real-Time Hindi Sign Language Prediction with Mediapipe Skeleton
----------------------------------------------------------------
- Opens webcam with ROI
- Detects hand using MediaPipe
- Draws skeleton with blue lines + green points
- Real-time prediction using trained CNN
- Stabilizes prediction using buffer
- Displays Hindi + English transliteration + confidence
"""

import os
import cv2
import numpy as np
import mediapipe as mp
from keras.models import load_model
from PIL import ImageFont, ImageDraw, Image
from collections import deque

# ---------------- CONFIG ----------------
MODEL_PATH = r"C:/Users/pankh/cnn_model99.h5"
FONT_PATH = r"C:/Users/pankh/NotoSansDevanagari-Regular.ttf"
ROI_WIDTH, ROI_HEIGHT = 300, 300  # width, height of ROI
TOP_MARGIN = 100  # distance from top
BUFFER_SIZE = 5   # number of frames to smooth prediction

# ---------------- Map index ‚Üí Hindi + transliteration ----------------
lettersMap = {
    0: ('_', '_'), 1: ('‡§Ö‡§Ç', 'aM'), 2: ('‡§Ö‡§É', 'aH'), 3: ('‡§Ü', 'aa'), 4: ('‡§á', 'i'), 5: ('‡§à', 'ii'), 6: ('‡§â', 'u'),
    7: ('‡§ä', 'uu'), 8: ('‡§ã', 'Ri'), 9: ('‡§è', 'e'), 10: ('‡§ê', 'ai'), 11: ('‡§ì', 'o'), 12: ('‡§î', 'au'),
    13: ('‡§ï', 'k'), 14: ('‡§ï‡•ç‡§∑', 'ksh'), 15: ('‡§ñ', 'kh'), 16: ('‡§ó', 'g'), 17: ('‡§ò', 'gh'), 18: ('‡§ô', 'ng'),
    19: ('‡§ö', 'ch'), 20: ('‡§õ', 'chh'), 21: ('‡§ú', 'j'), 22: ('‡§ú‡•ç‡§û', 'gy'), 23: ('‡§ù', 'jh'), 24: ('‡§û', 'ny'),
    25: ('‡§ü', 'T'), 26: ('‡§†', 'Th'), 27: ('‡§°', 'D'), 28: ('‡§¢', 'Dh'), 29: ('‡§£', 'N'), 30: ('‡§§', 't'),
    31: ('‡§§‡•ç‡§∞', 'tr'), 32: ('‡§•', 'th'), 33: ('‡§¶', 'd'), 34: ('‡§ß', 'dh'), 35: ('‡§®', 'n'), 36: ('‡§™', 'p'),
    37: ('‡§´', 'ph'), 38: ('‡§¨', 'b'), 39: ('‡§≠', 'bh'), 40: ('‡§Æ', 'm'), 41: ('‡§Ø', 'y'), 42: ('‡§∞', 'r'),
    43: ('‡§≤', 'l'), 44: ('‡§µ', 'v'), 45: ('‡§∂', 'sh'), 46: ('‡§∑', 'Sh'), 47: ('‡§∏', 's'), 48: ('‡§π', 'h')
}

# ---------------- Load model ----------------
print("üîπ Loading model...")
model = load_model(MODEL_PATH)
_, H, W, C = model.input_shape
print(f"‚úÖ Model loaded. Input shape: {(H, W, C)}")

# ---------------- Prediction buffer ----------------
pred_buffer = deque(maxlen=BUFFER_SIZE)

# ---------------- Mediapipe setup ----------------
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False,
                       max_num_hands=1,
                       min_detection_confidence=0.6,
                       min_tracking_confidence=0.6)

# Draw skeleton with blue lines + green points
def draw_skeleton(roi, hand_landmarks):
    h, w = roi.shape[:2]
    out = np.zeros_like(roi)
    pts = []
    for lm in hand_landmarks.landmark:
        x_px, y_px = int(lm.x * w), int(lm.y * h)
        pts.append((x_px, y_px))
    # Draw connections
    for c in mp_hands.HAND_CONNECTIONS:
        cv2.line(out, pts[c[0]], pts[c[1]], (255, 0, 0), 2)
    # Draw points
    for (x, y) in pts:
        cv2.circle(out, (x, y), 4, (0, 255, 0), -1)
    return out

# ---------------- Webcam ----------------
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    raise SystemExit("‚ùå Cannot open webcam")

font = ImageFont.truetype(FONT_PATH, 32)
print("üì∑ Webcam ready. Press 'q' to quit.")

while True:
    ret, frame = cap.read()
    if not ret:
        continue
    frame = cv2.flip(frame, 1)
    frame_h, frame_w = frame.shape[:2]

    # Shift ROI to upper-right
    x = frame_w - ROI_WIDTH - 10  # 10 px from right edge
    y = TOP_MARGIN
    w, h = ROI_WIDTH, ROI_HEIGHT

    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
    roi = frame[y:y+h, x:x+w]
    rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_roi)

    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        skel_img = draw_skeleton(roi, hand_landmarks)

        # Preprocess for CNN
        if C == 1:
            gray = cv2.cvtColor(skel_img, cv2.COLOR_BGR2GRAY)
            input_img = cv2.resize(gray, (W,H)).reshape(1,H,W,1)/255.0
        else:
            input_img = cv2.resize(skel_img, (W,H)).reshape(1,H,W,3)/255.0

        # Predict
        preds = model.predict(input_img, verbose=0)
        idx = int(np.argmax(preds[0]))
        pred_buffer.append(idx)
        most_common = max(set(pred_buffer), key=pred_buffer.count)
        hindi, translit = lettersMap.get(most_common, (f"Class {most_common}", f"Class {most_common}"))
        conf = float(np.max(preds[0]))

        # Overlay text
        img_pil = Image.fromarray(skel_img)
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 30), f"{hindi} ({translit}) [{conf:.2f}]", font=font, fill=(0,0,255))
        skel_img = np.array(img_pil)
        frame[y:y+h, x:x+w] = skel_img
    else:
        pred_buffer.clear()

    cv2.imshow("Hindi Sign Language Real-Time", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
