# CodeWithKiroHackathon

# ğŸ¤Ÿ Hindi Sign Language Recognition

A real-time **Hindi Sign Language Recognition System** that translates hand gestures into **Hindi characters**, represented as **English transliterations** or **phonetic sounds for pronunciation** with a model accuray of 92%.

This project was created for the **Code With Kiro Hackathon (Best Wildcard / Freestyle Category)**. It highlights how **AI can promote inclusivity, accessibility, and diversity** ensuring that technology benefits everyone, regardless of language or ability.

---

## ğŸŒŸ Introduction

Most sign language systems in AI focus on narrow datasets, leaving speakers of many languages underrepresented. To address this, I **self-created a dataset of 11,025 samples across 49 classes**.

* This dataset is **not publicly available**, as it was curated specifically for this project.
* Each sample was generated by capturing a hand gesture and processing it through **MediaPipe**.
* MediaPipe extracted **keypoints (landmarks)** and connected them into a **skeleton-like representation** of the hand (points + lines).
* Instead of raw hand photos, the dataset contains these **skeletonized images**, which makes the model:

  * Less biased by skin tone, background, or lighting.
  * More robust and generalizable.
  * Privacy-preserving.
 
    
An example of one the images used in dataset is -

![App Screenshot](sampleLetterImage.jpg)


This project shows that **AI is meant for everyone**, bringing people closer by bridging communication barriers through gestures and language.

---

## ğŸ› ï¸ Features

* ğŸ“· Real-time hand gesture recognition via webcam.
* ğŸ§  Custom CNN model trained on **49 Hindi gestures**.
* ğŸ”¡ **Two Output Modes:**

  * **Transliteration** â†’ Direct Hindi-to-English script (e.g., à¤• â†’ k).
  * **Phonetic Pronunciation** â†’ Hindi sound written in English letters (e.g., à¤• â†’ â€œkaâ€).
* ğŸ¯ Dataset created using **MediaPipe skeleton keypoints** instead of raw images.
* ğŸ§© Built with **MediaPipe, TensorFlow, and Python**.
* ğŸŒ Modular design ready for extension into other languages.

---

## ğŸ”¡ Hindi-to-English Phonetic Mapping (Examples)

| Hindi Letter | Transliteration | Phonetic Sound (English) |
| ------------ | --------------- | ------------------------ |
| à¤…            | a               | â€œuhâ€ (as in *cup*)       |
| à¤†            | aa              | â€œaaâ€ (as in *car*)       |
| à¤•            | k               | â€œkaâ€                     |
| à¤—            | g               | â€œgaâ€                     |
| à¤®            | m               | â€œmaâ€                     |
| à¤¶            | sh              | â€œshaâ€                    |

*(Full mapping for all 49 classes is implemented in the code.)*

---

## ğŸ§  Model Architecture & Performance

The recognition model is a **Convolutional Neural Network (CNN)** built specifically for skeletonized gesture classification.

**CNN Structure:**

* **Input Layer:** Skeletonized hand images from MediaPipe keypoints.
* **Conv2D Layer 1:** 32 filters, 3Ã—3 kernel, ReLU activation.
* **MaxPooling Layer 1:** 2Ã—2 pool size.
* **Conv2D Layer 2:** 64 filters, 3Ã—3 kernel, ReLU activation.
* **MaxPooling Layer 2:** 2Ã—2 pool size.
* **Flatten Layer**.
* **Dense Layer 1:** 128 neurons, ReLU activation.
* **Dropout Layer:** 0.5 (to reduce overfitting).
* **Dense Layer 2 (Output):** 49 neurons, Softmax activation (for 49 classes).

**Performance:**

* Achieved **92% accuracy** on the validation set.
* Balanced recognition across all 49 classes.
* Optimized for **real-time inference** using TensorFlow and GPU acceleration.

This accuracy demonstrates the **strength of using skeletonized MediaPipe data** combined with CNNs.

---

## ğŸš€ Future Improvements

Planned enhancements include:

* **Sentence Builder:** Combine recognized letters into words and full sentences.
* **Multi-Language Mode:** Output sentences in both Hindi & English.
* **Pronunciation Extension:** Expand phonetic mapping for better non-native accessibility.
* **Text-to-Speech (TTS):** Convert recognized sentences into natural speech.
* **Custom Gestures:** Let users define and map their own gestures.

---

## ğŸ¤– Built with Kiro

This project was developed using **Kiro**, an AI-powered development assistant that supported me throughout the entire lifecycle from idea conception to deployment. Kiro acted like a **teammate**, helping me **plan, code, debug, and refine**.

### ğŸ§  How I Used Kiro

#### 1. Vibe Coding for Ideation

* Shared my vision in plain language.
* Kiro brainstormed features like webcam-based recognition, modular dataset design, and phonetic mapping.
* Helped refine scope to **49 gestures, transliteration, and phonetics**.

#### 2. Spec-Driven Development

* Turned requirements into a **clear modular architecture**.
* Produced a **step-by-step task plan** for dataset â†’ training â†’ inference â†’ app.
* Ensured alignment with the **Best Wildcard / Freestyle Category** theme.

#### 3. Steering Files for Consistency

* Applied my coding style, folder structure, and library preferences.
* Maintained **clean, modular, and reusable** code across the project.

#### 4. Advanced Code Generation

* Generated core components:

  * CNN for gesture classification.
  * Transliteration and phonetic mapping.
  * Data preprocessing and augmentation.
  * Unit tests and debugging utilities.
* Helped tune the CNN to reach **92% accuracy** while staying real-time.

#### 5. Workflow Automation

* Tracked progress and allowed **session resumption** seamlessly.
* Reduced overhead by keeping task context alive.

#### 6. Iterative Debugging & Refinement

* Suggested fixes for Unicode rendering, dataset balancing, and overfitting.
* Recommended multiple strategies, improving both accuracy and reliability.

#### 7. Hackathon Readiness

* Assisted with **README drafting, documentation**, and impact storytelling.
* Strengthened the **social good narrative**, emphasizing inclusivity and diversity.

---

## ğŸ’¡ Impact

This project is an example of how AI can be a tool for **social good** not just efficiency.

* Celebrates **linguistic diversity**.
* Provides **assistive communication tools** for disabled communities.
* Bridges **cultural and linguistic gaps**.
* Shows that **AI belongs to everyone**, not just a select few.

By combining **technical innovation** with **social purpose**, this project demonstrates AI as a **force for empathy and inclusion**.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ dataset/                # Custom Hindi gesture dataset (11,025 samples, 49 classes, MediaPipe skeletonized)
â”œâ”€â”€ model/                  # CNN model + training scripts
â”œâ”€â”€ mediapipe_utils/        # Keypoint extraction scripts
â”œâ”€â”€ transliteration/        # Hindi to English mapping
â”œâ”€â”€ pronunciation/          # Hindi sounds in English phonetic style
â”œâ”€â”€ app.py                  # Main application script
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ“œ License

This project is open source and available under the [MIT License](LICENSE).

